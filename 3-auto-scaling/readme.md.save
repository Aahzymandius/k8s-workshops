# Auto-scaling in GKE

With the exception of Horizontal Pod Autoscaling, these are all GKE specific features and will require you to use a GKE cluster to follow along. 
For the sake of this tutorial, I recommend starting with a standard default cluster.

1. Node Pool auto-scaling
2. Node Auto-provisionning
3. Horizontal Pod Auto-scaling
4. Vertical pod Auto-scaling

## Cluster level auto-scaling

The first two types of auto-scaling occur at the Cluster level and allows your k8s cluster to add or remove hardware resources to meet the demands of fluctuating workloads. 
Both Node Pool Auto-Scaling and Node Auto Provisioning rely on resource requests configured in your containers. Without these values, the auto-scalers can't properly function.

### 1. Node Pool Auto-Scaling

Node Pool auto-scaling allows you to define a minimum and maximum number of nodes that the node pool can contain. These options are set at the node pool level and means that you can set different values per pool depending on your needs. 
Some node pools might contain static workloads so scaling isn't necesasry while a second node pool may be used for batch jobs and should only scale up on demand.

In more common use cases, the Node Pool Auto-Scaling will be set to ensure that there is always sufficient resources available to the cluster when new workloads (pods) are required. 
The auto-scaler waits until there are unschedulable, examines the node pools with auto-scaling enabled and then determines whether adding a new node will result in the pod being scheduled. 
This means that the scale up action is only considered when a pod is unschedulable. Workloads that exceed their requests that may lead to OOM or CPU pressure on the node will not directly cause the auto-scaler to trigger.

You can enable Node Pool Auto-Scaling at the node pool level either during node pool creation or by updating it. This can be done through the GUI by editing the Node Pool or by using the following command from the command line:

`gcloud container clusters update [cluster_name] --node-pool [node_pool_name] --enable-autoscaling --min-nodes [num_nodes] --max-nodes [num_nodes]`

Make sure that you have enough GCE resource quotas to handle the max number of nodes selected. 
For the following examples, set the min to 2 and the max to 6 

As soon as the update completes, the auto-scaler logic will kick in. If you have under utilized nodes, you may notice your cluster size begin to shrink.

To highlight the Node Pool Auto-scaling functionality, deploy the `node-scaling.yaml` 

`kubectl apply -f node-scaling.yaml`

If creating this deployment does not trigger a scale up action, add more pods by scaling up the deployment

`kubectl scale deploy -n scaling scalable-workload --replicas 50`

This should cause the cluster to reach the maximum of 6 nodes, even though there are still unschedulable pods. 
During the process, you can also view the auto-scaler confiMap to see the status and actions taken by the auto-scaler.

`kubectl describe cm -n kube-system cluster-autoscaler-status` 

The scale up process should happen pretty quickly, once a pod is unschedulable, the auto-scaler should detect it and scale up a new node. 
The scale down process, however, takes longer to detect. To scale down, we need to make sure the nodes are under utilized, so we'll scale down the deployment to 1. Describe the configMap periodically to watch the auto-scaler's logic.

`kubectl scale deploy -n scaling scalable-workload --replicas 1`


### 2. Node Auto-Provisionning

The node pool auto-scaler will increase the number of nodes available using the same node template as the pool. Whateveryou've selected as a machine type will continue being used. 
In contrast, Node Auto-Provisioning (NAP) will add an entirely new node pool to the cluster, it will determine the correct machine type to use to ensure that the new nodes accomodate the workloads you are adding. This is especially useful when using Vertical Pod Auto-scaling (section 4). 

Make sure to review the [GCP public docs for more details](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning) on NAP. We won't be covering all the different criteria that can trigger NAP to create new nodes, the document will provide you more details.  
Start by enabling NAP on your cluster:

`gcloud beta container clusters update [CLUSTER_NAME] --enable-autoprovisioning --max-cpu [num] --max-memory [num]`

Let's take a closer look at the two key flags:
- `--max-cpu` defines the maximum number of CPU coress your cluster can use. This includes any CPU currently in use as well as any CPU used by future nodes due to the Node Pool Auto-Scaler. 
- `--max-memory` defines how much RAM in Gb you are willing to assign to your cluster. 

Based on the above, note that NAP takes more planning to put into place than the Node Pool Auto-Scaler. You need to calculate how many resources are currently in use, how many may be used by auto-scaling and then determine how much more you are willing to assign to new nodes for NAP to use. 

Let's use our current cluster as an example:
- The default standard cluster will use  
